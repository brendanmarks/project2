{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1412,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import datasets\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
      "1109        0.0        0.0        0.0        9.0       15.0        2.0   \n",
      "940         0.0        3.0       12.0       12.0       14.0        4.0   \n",
      "192         0.0        1.0       10.0       15.0       16.0       13.0   \n",
      "260         0.0        0.0        0.0       12.0        4.0        0.0   \n",
      "1148        0.0        0.0        0.0        9.0       16.0        3.0   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "835         0.0        1.0        8.0       14.0       15.0        2.0   \n",
      "1216        0.0        2.0        9.0       15.0       16.0       15.0   \n",
      "1653        0.0        0.0        5.0       14.0       14.0        2.0   \n",
      "559         0.0        0.0        4.0       10.0       15.0       16.0   \n",
      "684         0.0        0.0        6.0       14.0       13.0        4.0   \n",
      "\n",
      "      pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_6  pixel_6_7  \\\n",
      "1109        0.0        0.0        0.0        0.0  ...       15.0        6.0   \n",
      "940         0.0        0.0        0.0        1.0  ...        8.0        0.0   \n",
      "192         3.0        0.0        0.0        5.0  ...        0.0        0.0   \n",
      "260         0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
      "1148        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
      "...         ...        ...        ...        ...  ...        ...        ...   \n",
      "835         0.0        0.0        0.0        2.0  ...        1.0        0.0   \n",
      "1216        2.0        0.0        0.0       11.0  ...        7.0        0.0   \n",
      "1653        0.0        0.0        0.0        2.0  ...        0.0        0.0   \n",
      "559         4.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
      "684         0.0        0.0        0.0        4.0  ...        1.0        0.0   \n",
      "\n",
      "      pixel_7_0  pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  \\\n",
      "1109        0.0        0.0        0.0        7.0       15.0       16.0   \n",
      "940         0.0        2.0       13.0       16.0       16.0       16.0   \n",
      "192         0.0        0.0       15.0       13.0        7.0        0.0   \n",
      "260         0.0        0.0        0.0       11.0        9.0        0.0   \n",
      "1148        0.0        0.0        0.0       12.0       12.0        0.0   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "835         0.0        1.0        9.0       12.0       13.0        9.0   \n",
      "1216        0.0        0.0       12.0       16.0       15.0        9.0   \n",
      "1653        0.0        0.0        9.0       13.0        0.0        0.0   \n",
      "559         0.0        0.0        6.0       16.0        4.0        0.0   \n",
      "684         0.0        0.0        5.0       16.0       16.0       11.0   \n",
      "\n",
      "      pixel_7_6  pixel_7_7  \n",
      "1109       16.0        6.0  \n",
      "940         2.0        0.0  \n",
      "192         0.0        0.0  \n",
      "260         0.0        0.0  \n",
      "1148        0.0        0.0  \n",
      "...         ...        ...  \n",
      "835         0.0        0.0  \n",
      "1216        1.0        0.0  \n",
      "1653        0.0        0.0  \n",
      "559         0.0        0.0  \n",
      "684         0.0        0.0  \n",
      "\n",
      "[1437 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "d = datasets.load_digits(as_frame=True)\n",
    "digits = d['data']\n",
    "digits_target = d['target']\n",
    "\n",
    "c = datasets.fetch_openml(name='credit-g', as_frame=True)\n",
    "credit = c['data']\n",
    "credit_target = c['target']\n",
    "\n",
    "digits, digits_test, digits_target, digits_target_test = train_test_split(digits, digits_target, test_size = 0.2, random_state = 0)\n",
    "\n",
    "credit, credit_test, credit_target, credit_target_test = train_test_split(credit, credit_target, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print (digits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
      "0           0.0        0.0        0.0        9.0       15.0        2.0   \n",
      "1           0.0        3.0       12.0       12.0       14.0        4.0   \n",
      "2           0.0        1.0       10.0       15.0       16.0       13.0   \n",
      "3           0.0        0.0        0.0       12.0        4.0        0.0   \n",
      "4           0.0        0.0        0.0        9.0       16.0        3.0   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "1432        0.0        1.0        8.0       14.0       15.0        2.0   \n",
      "1433        0.0        2.0        9.0       15.0       16.0       15.0   \n",
      "1434        0.0        0.0        5.0       14.0       14.0        2.0   \n",
      "1435        0.0        0.0        4.0       10.0       15.0       16.0   \n",
      "1436        0.0        0.0        6.0       14.0       13.0        4.0   \n",
      "\n",
      "      pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_7  pixel_7_0  \\\n",
      "0           0.0        0.0        0.0        0.0  ...        6.0        0.0   \n",
      "1           0.0        0.0        0.0        1.0  ...        0.0        0.0   \n",
      "2           3.0        0.0        0.0        5.0  ...        0.0        0.0   \n",
      "3           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
      "4           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
      "...         ...        ...        ...        ...  ...        ...        ...   \n",
      "1432        0.0        0.0        0.0        2.0  ...        0.0        0.0   \n",
      "1433        2.0        0.0        0.0       11.0  ...        0.0        0.0   \n",
      "1434        0.0        0.0        0.0        2.0  ...        0.0        0.0   \n",
      "1435        4.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
      "1436        0.0        0.0        0.0        4.0  ...        0.0        0.0   \n",
      "\n",
      "      pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  pixel_7_6  \\\n",
      "0           0.0        0.0        7.0       15.0       16.0       16.0   \n",
      "1           2.0       13.0       16.0       16.0       16.0        2.0   \n",
      "2           0.0       15.0       13.0        7.0        0.0        0.0   \n",
      "3           0.0        0.0       11.0        9.0        0.0        0.0   \n",
      "4           0.0        0.0       12.0       12.0        0.0        0.0   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "1432        1.0        9.0       12.0       13.0        9.0        0.0   \n",
      "1433        0.0       12.0       16.0       15.0        9.0        1.0   \n",
      "1434        0.0        9.0       13.0        0.0        0.0        0.0   \n",
      "1435        0.0        6.0       16.0        4.0        0.0        0.0   \n",
      "1436        0.0        5.0       16.0       16.0       11.0        0.0   \n",
      "\n",
      "      pixel_7_7  target  \n",
      "0           6.0       6  \n",
      "1           0.0       5  \n",
      "2           0.0       3  \n",
      "3           0.0       4  \n",
      "4           0.0       4  \n",
      "...         ...     ...  \n",
      "1432        0.0       3  \n",
      "1433        0.0       3  \n",
      "1434        0.0       7  \n",
      "1435        0.0       7  \n",
      "1436        0.0       8  \n",
      "\n",
      "[1437 rows x 65 columns]\n",
      "     pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
      "0          0.0        0.0       11.0       16.0       15.0        3.0   \n",
      "1          0.0        1.0       15.0       14.0        2.0        0.0   \n",
      "2          0.0        2.0       13.0       16.0       10.0        0.0   \n",
      "3          0.0        0.0        9.0        7.0        0.0        0.0   \n",
      "4          0.0        0.0        3.0       13.0        6.0        0.0   \n",
      "..         ...        ...        ...        ...        ...        ...   \n",
      "355        0.0        0.0        3.0        8.0       11.0       13.0   \n",
      "356        0.0        0.0        0.0        9.0       11.0        0.0   \n",
      "357        0.0        1.0        9.0       16.0       16.0       12.0   \n",
      "358        0.0        0.0        0.0        3.0       14.0       13.0   \n",
      "359        0.0        0.0        0.0        9.0       13.0       10.0   \n",
      "\n",
      "     pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_7  pixel_7_0  \\\n",
      "0          0.0        0.0        0.0        5.0  ...        0.0        0.0   \n",
      "1          0.0        0.0        0.0        6.0  ...        0.0        0.0   \n",
      "2          0.0        0.0        0.0       12.0  ...        0.0        0.0   \n",
      "3          0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
      "4          0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
      "..         ...        ...        ...        ...  ...        ...        ...   \n",
      "355       14.0        0.0        0.0        2.0  ...        0.0        0.0   \n",
      "356        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
      "357        1.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
      "358        3.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
      "359        1.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
      "\n",
      "     pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  pixel_7_6  \\\n",
      "0          0.0       13.0       13.0        8.0       13.0       16.0   \n",
      "1          1.0       15.0       16.0       12.0        1.0        0.0   \n",
      "2          1.0       13.0       16.0       16.0       16.0       16.0   \n",
      "3          0.0        7.0       14.0       16.0       12.0        1.0   \n",
      "4          0.0        3.0       13.0       15.0        8.0        0.0   \n",
      "..         ...        ...        ...        ...        ...        ...   \n",
      "355        0.0        2.0       12.0       13.0        2.0        0.0   \n",
      "356        0.0        0.0       11.0        7.0        0.0        0.0   \n",
      "357        0.0       10.0       16.0       11.0        4.0        0.0   \n",
      "358        0.0        0.0        3.0       13.0       15.0        2.0   \n",
      "359        0.0        0.0       10.0       16.0       12.0        0.0   \n",
      "\n",
      "     pixel_7_7  target  \n",
      "0          8.0       2  \n",
      "1          0.0       8  \n",
      "2          3.0       2  \n",
      "3          0.0       6  \n",
      "4          0.0       6  \n",
      "..         ...     ...  \n",
      "355        0.0       5  \n",
      "356        0.0       4  \n",
      "357        0.0       3  \n",
      "358        0.0       8  \n",
      "359        0.0       8  \n",
      "\n",
      "[360 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "# no preprocessing needed \n",
    "df = digits.transform(lambda x: x if (np.amax(x) == 0) else (x / np.amax(x)))\n",
    "digits_final = pd.concat([df, digits_target], axis=1)\n",
    "digits_final.reset_index(drop=True, inplace=True)\n",
    "print (digits_final)\n",
    "\n",
    "df2 = digits_test.transform(lambda x: x if (np.amax(x) == 0) else (x / np.amax(x)))\n",
    "digits_test_final = pd.concat([df2, digits_target_test], axis=1)\n",
    "digits_test_final.reset_index(drop=True, inplace=True)\n",
    "print (digits_test_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     duration  credit_amount  installment_commitment  residence_since  \\\n",
      "0    0.500000       0.155341                    1.00             0.75   \n",
      "1    0.333333       0.169507                    1.00             0.25   \n",
      "2    0.833333       0.402084                    1.00             0.50   \n",
      "3    0.208333       0.068606                    0.50             0.50   \n",
      "4    0.083333       0.084347                    0.25             0.50   \n",
      "..        ...            ...                     ...              ...   \n",
      "795  0.166667       0.058728                    1.00             1.00   \n",
      "796  0.375000       0.212495                    1.00             0.50   \n",
      "797  0.125000       0.207990                    0.25             1.00   \n",
      "798  0.250000       0.104646                    0.50             0.50   \n",
      "799  0.500000       0.535009                    0.25             0.75   \n",
      "\n",
      "          age  existing_credits  num_dependents  checking_status  purpose  \\\n",
      "0    0.400000              0.25             0.5                0        4   \n",
      "1    0.360000              0.25             0.5                1        4   \n",
      "2    0.320000              0.25             0.5                0        4   \n",
      "3    0.333333              0.25             0.5                0        4   \n",
      "4    0.320000              0.50             0.5                3        6   \n",
      "..        ...               ...             ...              ...      ...   \n",
      "795  0.640000              0.50             0.5                1        4   \n",
      "796  0.480000              0.25             1.0                0        0   \n",
      "797  0.853333              0.25             0.5                3        2   \n",
      "798  0.413333              0.50             0.5                0        3   \n",
      "799  0.413333              0.50             1.0                0        0   \n",
      "\n",
      "     credit_history  ...  employment  personal_status  other_parties  \\\n",
      "0                 2  ...           3                3              2   \n",
      "1                 3  ...           2                0              2   \n",
      "2                 3  ...           2                0              2   \n",
      "3                 0  ...           0                2              2   \n",
      "4                 1  ...           1                0              2   \n",
      "..              ...  ...         ...              ...            ...   \n",
      "795               4  ...           0                3              2   \n",
      "796               3  ...           0                3              2   \n",
      "797               3  ...           3                3              2   \n",
      "798               1  ...           2                3              2   \n",
      "799               2  ...           1                3              2   \n",
      "\n",
      "     property_magnitude  other_payment_plans  housing  job  own_telephone  \\\n",
      "0                     2                    1        0    1              0   \n",
      "1                     1                    1        1    1              0   \n",
      "2                     1                    1        1    0              0   \n",
      "3                     1                    1        2    1              0   \n",
      "4                     0                    1        2    1              1   \n",
      "..                  ...                  ...      ...  ...            ...   \n",
      "795                   0                    0        1    1              0   \n",
      "796                   0                    1        1    1              1   \n",
      "797                   3                    1        1    3              0   \n",
      "798                   3                    1        1    3              0   \n",
      "799                   1                    1        1    3              1   \n",
      "\n",
      "     foreign_worker  class  \n",
      "0                 1      1  \n",
      "1                 1      0  \n",
      "2                 1      0  \n",
      "3                 1      0  \n",
      "4                 1      1  \n",
      "..              ...    ...  \n",
      "795               1      0  \n",
      "796               1      0  \n",
      "797               1      1  \n",
      "798               1      0  \n",
      "799               1      1  \n",
      "\n",
      "[800 rows x 21 columns]\n",
      "     duration  credit_amount  installment_commitment  residence_since  \\\n",
      "0        0.60       0.272003                    1.00             0.75   \n",
      "1        0.15       0.245757                    0.25             0.50   \n",
      "2        0.30       0.172793                    0.75             1.00   \n",
      "3        0.20       0.137066                    1.00             0.25   \n",
      "4        1.00       0.712195                    0.50             1.00   \n",
      "..        ...            ...                     ...              ...   \n",
      "195      0.30       0.099828                    0.75             0.25   \n",
      "196      0.20       0.084370                    1.00             0.50   \n",
      "197      0.10       0.064033                    0.25             0.75   \n",
      "198      0.60       0.555548                    0.50             1.00   \n",
      "199      0.10       0.092477                    0.50             1.00   \n",
      "\n",
      "          age  existing_credits  num_dependents  checking_status  purpose  \\\n",
      "0    0.461538              0.25             0.5                1        3   \n",
      "1    0.400000              0.25             1.0                3        4   \n",
      "2    0.661538              0.25             0.5                3        3   \n",
      "3    0.415385              0.25             0.5                0        4   \n",
      "4    0.646154              0.25             0.5                3        4   \n",
      "..        ...               ...             ...              ...      ...   \n",
      "195  0.400000              0.25             0.5                3        6   \n",
      "196  0.369231              0.25             0.5                1        4   \n",
      "197  0.600000              0.50             0.5                0        8   \n",
      "198  0.646154              1.00             0.5                0        4   \n",
      "199  0.646154              0.25             1.0                3        6   \n",
      "\n",
      "     credit_history  ...  employment  personal_status  other_parties  \\\n",
      "0                 3  ...           4                3              2   \n",
      "1                 3  ...           0                3              1   \n",
      "2                 3  ...           0                3              2   \n",
      "3                 1  ...           2                3              2   \n",
      "4                 3  ...           3                3              2   \n",
      "..              ...  ...         ...              ...            ...   \n",
      "195               3  ...           2                0              2   \n",
      "196               3  ...           0                0              2   \n",
      "197               1  ...           1                0              2   \n",
      "198               2  ...           3                3              2   \n",
      "199               3  ...           3                3              2   \n",
      "\n",
      "     property_magnitude  other_payment_plans  housing  job  own_telephone  \\\n",
      "0                     1                    1        1    0              1   \n",
      "1                     3                    1        2    1              0   \n",
      "2                     3                    1        1    1              1   \n",
      "3                     0                    1        1    1              0   \n",
      "4                     1                    1        1    0              1   \n",
      "..                  ...                  ...      ...  ...            ...   \n",
      "195                   3                    1        1    1              0   \n",
      "196                   3                    1        1    3              0   \n",
      "197                   1                    1        1    3              0   \n",
      "198                   0                    1        1    0              1   \n",
      "199                   2                    0        0    1              1   \n",
      "\n",
      "     foreign_worker  class  \n",
      "0                 1      1  \n",
      "1                 0      1  \n",
      "2                 1      1  \n",
      "3                 1      1  \n",
      "4                 1      1  \n",
      "..              ...    ...  \n",
      "195               1      1  \n",
      "196               1      0  \n",
      "197               1      1  \n",
      "198               1      0  \n",
      "199               1      1  \n",
      "\n",
      "[200 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "#everything good with data except categorical to one hot needed \n",
    "credit_cat = ['checking_status', 'purpose', 'credit_history', 'savings_status', 'employment', 'personal_status', 'other_parties',\n",
    "             'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone', 'foreign_worker', 'class']\n",
    "\n",
    "def extract_columns (df, cols):\n",
    "    return df.loc[:, cols]\n",
    "\n",
    "def get_onehot (df, cat_feat):\n",
    "    categories = extract_columns(df, cat_feat)\n",
    "    le = LabelEncoder()\n",
    "    return categories.apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "merged_credit = pd.concat([credit, credit_target], axis=1)\n",
    "merged_credit.reset_index(drop=True, inplace=True)\n",
    "\n",
    "merged_credit_test = pd.concat([credit_test, credit_target_test], axis=1)\n",
    "merged_credit_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "one_hot_cols = get_onehot(merged_credit, credit_cat)\n",
    "credit_no_cat = merged_credit.drop(credit_cat, axis = 1) \n",
    "credit_no_cat = credit_no_cat.transform(lambda x: x if (np.amax(x) == 0) else (x / np.amax(x))) #normalize continuous data\n",
    "credit_final = pd.concat([credit_no_cat, one_hot_cols], axis=1)\n",
    "\n",
    "\n",
    "one_hot_cols_test = get_onehot(merged_credit_test, credit_cat)\n",
    "credit_test_no_cat = merged_credit_test.drop(credit_cat, axis=1)\n",
    "credit_test_no_cat = credit_test_no_cat.transform(lambda x: x if (np.amax(x) == 0) else (x / np.amax(x)))\n",
    "credit_test_final = pd.concat([credit_test_no_cat, one_hot_cols_test], axis=1)\n",
    "\n",
    "#le = LabelEncoder()\n",
    "\n",
    "#credit_target = le.fit_transform(credit_target)\n",
    "#credit_target_test = le.fit_transform(credit_target_test)\n",
    "\n",
    "print (credit_final)\n",
    "print (credit_test_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1416,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optimizer class implementing minibatch gradient descent for softmax regression. Can utilize either\n",
    "    gradient descent with momentum, or Adaptive Momentum Estimation (Adam), with optional L1 or L2\n",
    "    regularization.\n",
    "\"\"\"\n",
    "# Imports\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "        return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "\n",
    "# Shuffles the (x,y) instances, and outputs a list of minibatch_size sized (x,y) tuples\n",
    "def minibatch(x, y, minibatch_size):\n",
    "    x, y = shuffle(x, y)\n",
    "    minibatches = []\n",
    "    if not minibatch_size:\n",
    "        minibatches.append((x, y))\n",
    "    else:\n",
    "        for i in range(0, x.shape[0], minibatch_size):\n",
    "            x_mini = x[i:i+minibatch_size]\n",
    "            y_mini = y[i:i+minibatch_size]\n",
    "            minibatches.append((x_mini, y_mini))\n",
    "    return minibatches\n",
    "\n",
    "\n",
    "# Accuracy function\n",
    "def accuracy(y, yh):\n",
    "    B = np.where(yh.ravel() >= 0.5, 1, 0)\n",
    "    return np.mean(y.ravel() == B)\n",
    "\n",
    "\n",
    "# Returns the index of the maximum value in a list\n",
    "def argmax(lst):\n",
    "    return lst.index(max(lst))\n",
    "\n",
    "\n",
    "class GradientDescent:\n",
    "\n",
    "    \"\"\"\n",
    "    Class fields:\n",
    "       alphaa - learning rate of the optimizer\n",
    "       beta1 - momentum hyperparameter\n",
    "       max_iterations - gradient descent termination condition: maximum times iterated\n",
    "       max_no_change - gradient descent termination condition: maximum number of iterations\n",
    "                         without the validation error decreasing\n",
    "       minibatch_size - size of the minibatch to use (default value of 0 indicates use full batch)\n",
    "       cost_fn - optional cost function, if included optimizer will calculate and store the\n",
    "                    training and validation cost at each iteration\n",
    "       adaptive - if true, optimizer uses Adam (Adaptive Moment Estimation) rather than gradient\n",
    "                    descent with momentum\n",
    "       beta2 - 2nd hyperparameter for Adam (if using)\n",
    "       epsilon - 3rd hyperparameter for Adam (if using), just to avoid numerical issues\n",
    "       regularize - determines regularization used (if any): 0 indicates no regularization, 1 or 2\n",
    "                      indicate L1 or L2 regularization respectively\n",
    "       lambdaa - regularization coefficient if used\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(self, alphaa=0.01, beta1=0.9, max_iterations=1e4, max_no_change=20, minibatch_size=0,\n",
    "                 cost_fn=None, adaptive=False, beta2=0.999, epsilon=1e-8, regularize=0, lambdaa=0.1):\n",
    "\n",
    "        self.alphaa = alphaa\n",
    "        self.beta1 = beta1\n",
    "        self.max_iterations = max_iterations\n",
    "        self.max_no_change = max_no_change\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.cost_fn = cost_fn\n",
    "\n",
    "        self.adaptive = adaptive\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.regularize = regularize\n",
    "        self.lambdaa = lambdaa\n",
    "\n",
    "        self.accuracy_tr = []\n",
    "        self.accuracy_val = []\n",
    "        self.weight_history = []\n",
    "        if self.cost_fn:\n",
    "            self.cost_tr = []\n",
    "            self.cost_val = []\n",
    "\n",
    "    # Run method - delegates work to one of 2 helper methods, dependent on if Adam is being used or not\n",
    "    def run(self, x_tr, y_tr, x_val, y_val, w):\n",
    "        if self.adaptive:\n",
    "            return self.adam(x_tr, y_tr, x_val, y_val, w)\n",
    "        else:\n",
    "            return self.momentum(x_tr, y_tr, x_val, y_val, w)\n",
    "\n",
    "    # Gradient Descent with Momentum\n",
    "    def momentum(self, x_tr, y_tr, x_val, y_val, w):\n",
    "        t = 1\n",
    "        i = 0\n",
    "        delta_w = 0\n",
    "\n",
    "        self.accuracy_tr.append(accuracy(y_tr, softmax(np.dot(x_tr, w))))\n",
    "        self.accuracy_val.append(accuracy(y_val, softmax(np.dot(x_val, w))))\n",
    "        self.weight_history.append(w)\n",
    "\n",
    "        while i < self.max_no_change and t < self.max_iterations:\n",
    "            for (x_mini, y_mini) in minibatch(x_tr, y_tr, self.minibatch_size):\n",
    "                grad = self.gradient(x_mini, y_mini, w)\n",
    "                delta_w = (self.beta1 * delta_w) + ((1 - self.beta1) * grad)\n",
    "                w -= self.alphaa * delta_w\n",
    "\n",
    "            self.weight_history.append(w)\n",
    "            tr_pred = softmax(np.dot(x_tr, w))\n",
    "            pred = softmax(np.dot(x_val, w))\n",
    "            self.accuracy_tr.append(accuracy(y_tr, tr_pred))\n",
    "            self.accuracy_val.append(accuracy(y_val, pred))\n",
    "            if self.cost_fn:\n",
    "                self.cost_tr.append(self.cost_fn(y_tr, tr_pred))\n",
    "                self.cost_val.append(self.cost_fn(y_val, pred))\n",
    "\n",
    "            if not self.accuracy_val[-1] < self.accuracy_val[-2]:\n",
    "                i += 1\n",
    "            else:\n",
    "                i = 0\n",
    "            t += 1\n",
    "        return self.weight_history[argmax(self.accuracy_val)]\n",
    "\n",
    "    # Adaptive Moment Estimation\n",
    "    def adam(self, x_tr, y_tr, x_val, y_val, w):\n",
    "        t = 1\n",
    "        i = 0\n",
    "        m = 0\n",
    "        s = 0\n",
    "\n",
    "        self.accuracy_tr.append(accuracy(y_tr, softmax(np.dot(x_tr, w))))\n",
    "        self.accuracy_val.append(accuracy(y_val, softmax(np.dot(x_val, w))))\n",
    "        self.weight_history.append(w)\n",
    "\n",
    "        while i < self.max_no_change and t < self.max_iterations:\n",
    "            for (x_mini, y_mini) in minibatch(x_tr, y_tr, self.minibatch_size):\n",
    "                grad = self.gradient(x_mini, y_mini, w)\n",
    "                m = (self.beta1 * m) + ((1 - self.beta1) * grad)\n",
    "                s = (self.beta2 * s) + ((1 - self.beta2) * np.power(grad, 2))\n",
    "                mh = m / (1 - np.power(self.beta1, t))\n",
    "                sh = s / (1 - np.power(self.beta2, t))\n",
    "                w -= self.alphaa * mh * grad / (np.sqrt(sh) + self.epsilon)\n",
    "\n",
    "            self.weight_history.append(w)\n",
    "            tr_pred = softmax(np.dot(x_tr, w))\n",
    "            pred = softmax(np.dot(x_val, w))\n",
    "            self.accuracy_tr.append(accuracy(y_tr, tr_pred))\n",
    "            self.accuracy_val.append(accuracy(y_val, pred))\n",
    "            if self.cost_fn:\n",
    "                self.cost_tr.append(self.cost_fn(y_tr, tr_pred))\n",
    "                self.cost_val.append(self.cost_fn(y_val, pred))\n",
    "\n",
    "            if not self.accuracy_val[-1] < self.accuracy_val[-2]:\n",
    "                i += 1\n",
    "            else:\n",
    "                i = 0\n",
    "            t += 1\n",
    "        return self.weight_history[argmax(self.accuracy_val)]\n",
    "\n",
    "    # Helper method to calculate gradient (and add regularization penalty if any)\n",
    "    def gradient(self, x, y, w):\n",
    "        n, d = x.shape\n",
    "        yh = softmax(np.dot(x, w))\n",
    "\n",
    "        grad = np.dot(x.T, yh - y) / n\n",
    "        if self.regularize == 1:\n",
    "            grad[1:] += self.lambdaa * np.sign(w[1:])\n",
    "        elif self.regularize == 2:\n",
    "            grad[1:] += self.lambdaa * w[1:]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1417,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from .._base import _BaseClassifier\n",
    "#from .._base import _BaseMultiClass\n",
    "\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "\n",
    "    \"\"\"Softmax regression classifier.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, max_iters=50,\n",
    "                 l2=0.0,\n",
    "                 minibatches=1,\n",
    "                 n_classes=None):\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.l2 = l2\n",
    "        self.minibatches = minibatches\n",
    "        self.n_classes = n_classes\n",
    "        self.random_seed = 0\n",
    "\n",
    "\n",
    "    def fit(self, X_tr, y_tr, X_val, y_val, gd):\n",
    "        \n",
    "        if self.random_seed is not None:\n",
    "            np.random.seed(self.random_seed)\n",
    "  \n",
    "        if self.n_classes is None:\n",
    "            self.n_classes = np.max(y_tr) + 1\n",
    "        self._n_features = X_tr.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"Initialize weight coefficients.\"\"\"\n",
    "        np.random.seed(self.random_seed)\n",
    "        self.w_ = np.random.normal(loc=0.0, scale=0.01, size=(self._n_features, self.n_classes)).astype('float64')\n",
    "        self.cost_ = []\n",
    "\n",
    "        y_tr_enc = self._one_hot(y=y_tr, n_labels=self.n_classes, dtype=np.float)\n",
    "        y_val_enc = self._one_hot(y=y_val, n_labels=self.n_classes, dtype=np.float)\n",
    "\n",
    "     \n",
    "        for i in range(self.max_iters):\n",
    "           \n",
    "            \n",
    "            #GRADIENT DESCENT line below (make sure to split X, y into training and validation sets)\n",
    "            \n",
    "            self._w = gd.run(X_tr,y_tr_enc,X_val,y_val_enc,self.w_)\n",
    "            \n",
    "            #COMMENT THE FOR LOOP BELOW\n",
    "\n",
    "            \"\"\" \n",
    "            for idx in self._yield_minibatches_idx(\n",
    "                    n_batches=self.minibatches,\n",
    "                    data_ary=y,\n",
    "                    shuffle=True):\n",
    "                # givens:\n",
    "                # w_ -> n_feat x n_classes\n",
    "                # b_  -> n_classes\n",
    "                \n",
    "                # net_input, softmax and diff -> n_samples x n_classes:\n",
    "                net = X[idx].dot(self.w_) #net_input\n",
    "                softm = self.softmax(net) \n",
    "                diff = softm - y_enc[idx]\n",
    "                mse = np.mean(diff, axis=0)\n",
    "\n",
    "                # gradient -> n_features x n_classes\n",
    "                grad = np.dot(X[idx].T, diff)\n",
    "                \n",
    "                # update in opp. direction of the cost gradient\n",
    "                self.w_ -= (self.eta * grad +\n",
    "                            self.eta * self.l2 * self.w_)  \n",
    "            \n",
    "            \"\"\"  \n",
    "            # compute cost of the whole epoch\n",
    "            net = X_tr.dot(self.w_)\n",
    "            softm = self.softmax(net)\n",
    "            cross_ent = self.cross_entropy(output=softm, y_target=y_tr_enc)\n",
    "            cost = self.cost(cross_ent)\n",
    "            self.cost_.append(cost)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        #Predict targets from X.\n",
    "\n",
    "        net = X.dot(self.w_)\n",
    "        probas = self.softmax(net)\n",
    "        return probas.argmax(axis=1)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "\n",
    "    def cross_entropy(self, output, y_target):\n",
    "        return - np.sum(np.log(output) * (y_target), axis=1)\n",
    "\n",
    "    def cost(self, cross_entropy):\n",
    "        L2_term = self.l2 * np.sum(self.w_ ** 2)\n",
    "        cross_entropy = cross_entropy + L2_term\n",
    "        return 0.5 * np.mean(cross_entropy)\n",
    "\n",
    "\n",
    "    def _one_hot(self, y, n_labels, dtype):\n",
    "        mat = np.zeros((len(y), n_labels))\n",
    "        for i, val in enumerate(y):\n",
    "            mat[i, val] = 1\n",
    "        return mat.astype(dtype)    \n",
    "    \n",
    "    def _yield_minibatches_idx(self, n_batches, data_ary, shuffle=True):\n",
    "            indices = np.arange(data_ary.shape[0])\n",
    "\n",
    "            if shuffle:\n",
    "                indices = np.random.permutation(indices)\n",
    "            if n_batches > 1:\n",
    "                remainder = data_ary.shape[0] % n_batches\n",
    "\n",
    "                if remainder:\n",
    "                    minis = np.array_split(indices[:-remainder], n_batches)\n",
    "                    minis[-1] = np.concatenate((minis[-1],\n",
    "                                                indices[-remainder:]),\n",
    "                                               axis=0)\n",
    "                else:\n",
    "                    minis = np.array_split(indices, n_batches)\n",
    "\n",
    "            else:\n",
    "                minis = (indices,)\n",
    "\n",
    "            for idx_batch in minis:\n",
    "                yield idx_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1418,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1416-f67499bc9576>:32: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  return np.mean(y.ravel() == B)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhV1b3/8fc3JxMZCWQAEkgIo8yjyqDSqgWnOrWotOrPiTpVbW+1tt72drhP23utiq2tLWpr7U9rrRPWWhFHVIoQ5knmAGFIQpgyEMiw7h/ngCkSpmRnJ9mf1/Oc5+Tss3P4LjeeD3vtvdYy5xwiIhJcUX4XICIi/lIQiIgEnIJARCTgFAQiIgGnIBARCbhovws4Wenp6S4vL8/vMkRE2pQFCxbsdM5lHO29NhcEeXl5FBQU+F2GiEibYmabGntPXUMiIgGnIBARCTgFgYhIwCkIREQCTkEgIhJwCgIRkYBTEIiIBFxggmB9aQU//vsKaurq/S5FRKRVCUwQbCqr5I8fF/LGsu1+lyIi0qoEJggm9M0kPyORpz7aiBbjERH5TGCCICrKuGFcT5YW7WXBpt1+lyMi0moEJggArhyRTWqHGJ76aKPfpYiItBqBCoKE2GimnNGDmSt2sGVXld/liIi0CoEKAoDrxuQSZcbTcwr9LkVEpFUIXBB0Te3AhYO78tf5WyivrvG7HBER3wUuCABuGt+TigO1vFBQ5HcpIiK+C2QQDO3ekVG5afzx443UaoCZiARcIIMAYOrZ+RTt3s8/NMBMRAIusEFw3mlZ9M5M4ncfbNAAMxEJtMAGQVSU8Y2z81m1fR8frCn1uxwREd8ENggALh2WTdfUeB5/f73fpYiI+CbQQRAbHcVN43vyycZdLNqsaSdEJJgCHQQAV5/eg9QOMfzuA50ViEgwBT4IkuKiuW5MLm+tLGZdSYXf5YiItLjABwHA9WPziA1FMX22zgpEJHgUBEB6UhyTR3XnlUVb2bG32u9yRERalIIgYurZ+dQ7ePLDDX6XIiLSohQEEd07JXDJkK48+8lmyioO+F2OiEiLURA0cOcXe1NdW8eTWrhGRAJEQdBA78xkLhrclWfmFLK78qDf5YiItAgFwRHuOrcPlQfrtJyliASGguAIfbOSuXBwF56eU8jeKi1cIyLtn4LgKO46tw8VB2p56mOdFYhI+6cgOIr+XVKYNLALf/x4I3v366xARNo3BUEjvnlub8qra3n640K/SxER8ZSCoBEDu6Vy/oAsnvpogxa5F5F2TUFwDHd9sQ/7dFYgIu2cZ0FgZn8wsxIzW97I+2ZmvzKzdWa21MxGeFXLqRqck8p5p2Ux/cMNuoNIRNotL88IngYmHeP9C4A+kcdU4HEPazll//GlvlQcqOX3mplURNopz4LAOTcb2HWMXS4FnnFhc4GOZtbVq3pO1WldU/jy0G788eNCSso1M6mItD9+XiPIBrY0eF0U2fY5ZjbVzArMrKC0tOUXmv/WeX05WFfPb95d1+J/toiI1/wMAjvKNne0HZ1z051zo5xzozIyMjwu6/Py0hOZPKo7z83bzJZdVS3+54uIeMnPICgCujd4nQNs86mW47r73D6YGY++s9bvUkREmpWfQfAacF3k7qEzgb3Oue0+1nNMXVLjuX5MLi8vLGJdSbnf5YiINBsvbx/9C/AvoJ+ZFZnZTWZ2q5ndGtnlDWADsA54Arjdq1qay20TetMhJsTDs9b4XYqISLOJ9uqDnXPXHOd9B9zh1Z/vhU6Jsdx8Vj6PvrOWZUV7GZyT6ndJIiJNppHFJ+nms3qSlhDDz/+5inCWiYi0bQqCk5QcH8Pd5/Zhzvoy3l/d8reyiog0NwXBKZhyRi490xP52RurqK2r97scEZEmURCcgtjoKL47qT9rSyp4oaDI73JERJpEQXCKJg7MYnReGg/PWkPFgVq/yxEROWUKglNkZnz/wtPYWXGA6R9oQjoRabsUBE0wvEcalwztxvQPN7BjryakE5G2SUHQRPdN7Ed9PTz01mq/SxEROSUKgibq3imBG8bl8eLCIlZs2+t3OSIiJ01B0Axu/0JvOnaI4cd/X6lBZiLS5igImkFqhxjundifeRt38felrXbePBGRo1IQNJOrRndnUHYKP/vHKqoO6nZSEWk7FATNJBRl/PjLA9mxr5rfvKeVzESk7VAQNKORuZ24Yng2T8zeyKaySr/LERE5IQqCZnb/Bf2JCRk/fX2l36WIiJwQBUEzy0yJ565z+/D2qhLeW13idzkiIselIPDADeN6kp+eyE/+vpKDtZqdVERaNwWBB2Kjo/jhJQPYuLOS6bM1D5GItG4KAo9M6JfJhYO78Ot311G4UxeORaT1UhB46L8uGUhMKIofzFiuEcci0mopCDyUlRLPfZP68eHanby2ZJvf5YiIHJWCwGNfOyOXoTmp/PT1leytqvG7HBGRz1EQeCwUZfzsisHsrqrhf2Z+6nc5IiKfoyBoAQO7pXLjuDye+2QzCzbt8rscEZF/oyBoIfec15duqfF8/+XlGlsgIq2KgqCFJMZF85NLB7G6uJzH39fYAhFpPRQELei8AVlcOqwbv353Lau27/O7HBERQEHQ4n50yUA6JsRw74tLqKlTF5GI+E9B0MLSEmP578sGsXzrPn7/gbqIRMR/CgIfTBrUlYuGdOXRd9ayeke53+WISMApCHzyky8PJDk+3EVUqy4iEfGRgsAnnZPi+MmlA1latJcnPtzodzkiEmAKAh9dNLgrFwzqwiOz1vDpDt1FJCL+UBD4yMz46WWDSOkQzT3PL6a6ps7vkkQkgBQEPktPiuN/vzKET3eU89Bbq/0uR0QCSEHQCnyxfxZfP7MHT360kTnrdvpdjogEjIKglXjgwgH0TE/kP/62RNNVi0iL8jQIzGySma02s3Vmdv9R3k81s7+b2RIzW2FmN3hZT2vWITbEtKuGUVp+gAdeXaYVzUSkxXgWBGYWAn4DXAAMAK4xswFH7HYHsNI5NxSYADxkZrFe1dTaDcnpyD3n9eH1pduZsVgrmolIy/DyjOB0YJ1zboNz7iDwPHDpEfs4INnMDEgCdgG1HtbU6t02oTejctP4z1eXs6lMi96LiPe8DIJsYEuD10WRbQ09BpwGbAOWAXc75z43zNbMpppZgZkVlJaWelVvqxCKMqZdPYwogzufW8SBWt1SKiLe8jII7Cjbjuz4nggsBroBw4DHzCzlc7/k3HTn3Cjn3KiMjIzmr7SVyUlL4MGvDmXZ1r38/A0tbyki3vIyCIqA7g1e5xD+l39DNwAvu7B1wEagv4c1tRkTB3bhhnF5PD2nkDeX7/C7HBFpx7wMgvlAHzPrGbkAfDXw2hH7bAbOBTCzLKAfsMHDmtqU711wGkNyUrnvxSVs2VXldzki0k55FgTOuVrgTmAmsAp4wTm3wsxuNbNbI7v9FBhrZsuAd4DvOuc0oioiNjqKx64ZgXNw518Waa1jEfGEtbX71UeNGuUKCgr8LqNF/XPZdm57diE3je/JDy4+8g5cEZHjM7MFzrlRR3tPI4vbgAsGd+X6Mbk89dFGXl+q8QUi0rwUBG3EAxcNYFRuGve9uFSrmolIs1IQtBGx0VH89msjSIqL5ht/LmDvfs1HJCLNQ0HQhmSmxPP410ewdc9+7nl+EfX1bev6joi0TgqCNmZkbid+ePEA3ltdyrR31vpdjoi0AwqCNujrZ+bylZE5/OqdtcxaWex3OSLSxikI2iAz478vG8SQnFTueX6R1jsWkSY5oSAwsz+fyDZpOfExIaZfO4qk+GhuerqAnRUH/C5JRNqoEz0jGNjwRWStgZHNX46cjC6p8Tx53WjKKg8w9ZkCqms0U6mInLxjBoGZfc/MyoEhZrYv8igHSoAZLVKhHNPgnFQemTyMhZv38N2XlmplMxE5accMAufcz51zycCDzrmUyCPZOdfZOfe9FqpRjuOCwV25d2I/ZizexmPvrvO7HBFpY060a+h1M0sEMLOvm9nDZpbrYV1ykm6f0Isrhmfz0Kw1moZCRE7KiQbB40CVmQ0F7gM2Ac94VpWcNDPj51cOZnReGt9+YQmfbCjzuyQRaSNONAhqXbjz+VLgUefco0Cyd2XJqYiLDvHEdaPo0SmBm58p0JxEInJCTjQIys3se8C1wD8idw3FeFeWnKqOCbH86cbTSYgNcf0f5rFtz36/SxKRVu5Eg+Aq4ABwo3NuB+FF6B/0rCppkuyOHXj6htOpPFDL9X+Yx94qTVAnIo07oSCIfPk/C6Sa2cVAtXNO1whasdO6pvD760ayqayKWzTGQESO4URHFk8G5gFfBSYDn5jZV7wsTJpubK90Hpo8lHmFu7jzuUXU1GmpSxH5vOgT3O8BYLRzrgTAzDKAt4EXvSpMmsclQ7uxu+ogP5yxgu/8bQkPTx5GKMr8LktEWpETDYKoQyEQUYYmrGszrhuTR8WBWv73zdUkxIb42eWDMVMYiEjYiQbBm2Y2E/hL5PVVwBvelCReuH1Cbyqqa/nt++tJjI3mgYtOUxiICHCcIDCz3kCWc+5eM7sCGA8Y8C/CF4+lDbl3Yj8qD9Ty5EcbSYqP5p7z+vpdkoi0Asc7I5gGfB/AOfcy8DKAmY2KvHeJp9VJszIz/uuSgVQerGPa22uJiw5x24RefpclIj47XhDkOeeWHrnROVdgZnmeVCSeiooyfnHFYA7U1vM/b35KvXPc8YXefpclIj46XhDEH+O9Ds1ZiLSc6FAUj0weSpTBgzNXAygMRALseEEw38xucc490XCjmd0ELPCuLPFadCiKh746FCMcBvX1jm+e28fvskTEB8cLgnuAV8zsa3z2xT8KiAUu97Iw8V50KIqHJg8jyoyHZq2h3sHd5ykMRILmmEHgnCsGxprZF4BBkc3/cM6963ll0iJCUcaDXx0KBo+8vYYDtXXcO7Gfbi0VCZATGkfgnHsPeM/jWsQnoSjjwa8MJS46it++v57y6lp+/OWBRGkEskggnOiAMmnnQlHGzy4fTEp8DL+fvYHy6hoe/OpQYkIaQC7S3ikI5DAz4/4L+pPSIYYHZ66m4kAdj00ZTnxMyO/SRMRD+uee/Bsz444v9Oanlw7k7VXF3PDH+ZRXaz0DkfZMQSBHde2YPKZdNYz5hbuY/Pu5FO+r9rskEfGIgkAaddnwbJ76f6PZXFbJ5b/5mLXFWgNZpD1SEMgxndM3g79+Yww19Y4rH5/DJxvK/C5JRJqZgkCOa1B2Kq/cPpaM5DiufWoery/d5ndJItKMFARyQnLSEnjptrEM7Z7Knc8t4rF31+Kc87ssEWkGngaBmU0ys9Vmts7M7m9knwlmttjMVpjZB17WI03TMSGWP990BpcN68Yv31rDt/66mOqaOr/LEpEm8mwcgZmFgN8A5wNFhCewe805t7LBPh2B3wKTnHObzSzTq3qkecTHhHjkqmH0yUrmwZmrKSyrYvp1I8lMPtZEtSLSmnl5RnA6sM45t8E5dxB4Hrj0iH2mAC875zYDHLEusrRSh8Ya/O7rI1m9o5zLHvuYFdv2+l2WiJwiL4MgG9jS4HVRZFtDfYE0M3vfzBaY2XVH+yAzm2pmBWZWUFpa6lG5crImDerC324dgwOufHwOMxZv9bskETkFXgbB0WYsO/LqYjQwErgImAj8wMw+t5Cuc266c26Uc25URkZG81cqp2xQdioz7hzHkOyO3P38Yn702goO1tb7XZaInAQvg6AI6N7gdQ5w5H2HRcCbzrlK59xOYDYw1MOaxAOZyfE8e8sZ3DS+J0/PKWTKExqJLNKWeBkE84E+ZtbTzGKBq4HXjthnBnCWmUWbWQJwBrDKw5rEIzGhKH5w8QB+fc1wVm7fx0W/+kiDz0TaCM+CwDlXC9wJzCT85f6Cc26Fmd1qZrdG9lkFvAksBeYBTzrnlntVk3jvkqHdePWOcaTERzPlyU948sMNGm8g0spZW/ufdNSoUa6goMDvMuQ49lXX8J0XlvDWymLOOy2LB78yhLTEWL/LEgksM1vgnBt1tPc0slg8kRIfw++vHckPLx7A7DWlXPDoh/xrvbqKRFojBYF4xsy4cXxPXr59LAmxIaY8OZeH31pNbZ3uKhJpTRQE4rlB2an8/ZvjuXJEDr96dx1XT5/L1j37/S5LRCIUBNIiEuOi+eVXh/Lo1cP4dEc5F0ybzYzFW3UhWaQVUBBIi7p0WDb/uGs8vTOTuPv5xdz+7ELKKg74XZZIoCkIpMXldk7kb7eO5buT+vPOqhImTpvNzBU7/C5LJLAUBOKLUJRx24RevPbNcWQmx/ONPy/g2y8sZu/+Gr9LEwkcBYH4qn+XFF69Yxx3nduHGYu3MfGR2byzqtjvskQCRUEgvouNjuLb5/flldvHktIhmpv+VMAdzy2kpFzzFYm0BAWBtBpDcjry+jfP4tvn92XWimLOe+gD/jp/s+4sEvGYgkBaldjoKO46tw9v3H0W/buk8N2XlnHNE3PZuLPS79JE2i0FgbRKvTOTeH7qmfzs8sGs2LaPidNmM+3tNVojWcQDCgJptaKijCln9OCdb5/D+QOymPb2Ws5/5ANmrSxWd5FIM1IQSKuXmRLPb6aM4LmbzyA+OsQtzxRww9PzKVR3kUizUBBImzG2dzpv3H0W/3nRaRQU7uZLj8zmlzNXU3Ww1u/SRNo0BYG0KTGhKG4+K593v3MOFw/tymPvreMLv3yfFwq2UFev7iKRU6EgkDYpMzmehycP46XbxtCtYwfue3EpF//6Iz5au9Pv0kTaHAWBtGkjczvx8m1jeWzKcCoO1PD1pz7hhj/OY01xud+libQZCgJp88yMi4d04+1vn8P3L+xPwabdTJo2m/tfWso2rXsgclxas1janV2VB/nVO2t57pPNAEw5owd3fKE3GclxPlcm4p9jrVmsIJB2a+ue/fzq7bW8uLCI2FAUN4zL4xtn9yI1Icbv0kRanIJAAm1DaQXT3l7La0u2kRwfzdSz8rlhfE+S4qL9Lk2kxSgIRIBV2/fx0FtreHtVMWkJMdw0vifXjc0jJV5nCNL+KQhEGli0eTe/fncd735aQnJ8NNePyePG8T3plBjrd2kinlEQiBzF8q17+e376/jn8h3ER4f42hk9mHp2Ppkp8X6XJtLsFAQix7CupJzfvreeGUu2EYoyJo/K4Zaz8sntnOh3aSLNRkEgcgI2l1Xx+AfreXHBFmrrHRMHdOGWs3syMreT36WJNJmCQOQkFO+r5pl/FfL/525m7/4ahvfoyM3j85k4MIvokMZgStukIBA5BVUHa3lxQRFPfbSRTWVV5KR14MZxPZk8urtuPZU2R0Eg0gR19Y5ZK4t56qMNzC/cTVJcNFeMyObaM3Ppk5Xsd3kiJ0RBINJMFm/ZwzNzCnl96XYO1tVzZn4nrhuTx/kDsohRt5G0YgoCkWZWVnGAvxZs4dm5m9m6Zz9ZKXFMOT2Xa07vrttPpVVSEIh4pK7e8e6nJTzzr0I+XLuTUJRxbv9MrhrdnXP6ZujisrQaxwoCXfESaYJQlHH+gCzOH5DFhtIK/jp/Cy8tLOKtlcVkpcRx5YgcJo/qTl66xiRI66UzApFmVlNXzzurSnihYAvvry6h3sGZ+Z24anR3LhjUlfiYkN8lSgCpa0jEJzv2VvPigi28UFDE5l1VJMdHc8nQblw2LJtRuWlERZnfJUpAKAhEfFZf75i7sYwX5m9h5opi9tfUkd2xA5cN78blw7PpnanbUMVbvgWBmU0CHgVCwJPOuV80st9oYC5wlXPuxWN9poJA2rrKA7W8tXIHryzaxkdrS6l3MCg7hcuGZfPlod1015F4wpcgMLMQsAY4HygC5gPXOOdWHmW/WUA18AcFgQRJSXk1ry/ZzquLt7K0aC9RBmN7pXPRkK5MHNhFU2NLs/ErCMYAP3LOTYy8/h6Ac+7nR+x3D1ADjAZeVxBIUK0rqWDG4q28tmQbm8qqCEUZY3t15sLBCgVpOr+C4CvAJOfczZHX1wJnOOfubLBPNvAc8EXgKRoJAjObCkwF6NGjx8hNmzZ5UrNIa+CcY8W2fbyxbDtvLNtOYSQUxuQfCoUsOifF+V2mtDF+jSM42u0QR6bONOC7zrk6s8bvnnDOTQemQ/iMoNkqFGmFzIxB2akMyk7l3on9WLn9UCjs4PuvLOMHM5Zzel6nw+MXundK8LtkaeN87Roys418FhjpQBUw1Tn3amOfq64hCSrnHKu2l/PGsu28tXIHa4orAOjfJflwKAzqlqpbUuWo/OoaiiZ8sfhcYCvhi8VTnHMrGtn/aXSNQOSEbSqrZNbKYmatLGZ+4S7qHWSlxHHeaeFQGNOrM3HRGrwmYb50DTnnas3sTmAm4dtH/+CcW2Fmt0be/51Xf7ZIEOR2TuTms/K5+ax8dlce5L3VJcxaWcwri7by7CebSYwNMa53Ouf0y2BCv0yyO3bwu2RppTSgTKSdqa6p418bypi1spgPVpeydc9+APpkJnFO33AojO6ZprOFgNHIYpGAcs6xvrSC91eX8sGaUj7ZsIuDdfV0iAkxtldnzumXwTl9M8jtrEnx2jvNPioSUGZG78xkemcmc/NZ+VQdrGXuhjLeX13K+6tLeefTEgBy0jowrlc6Y3t3ZkyvzmQma3RzkOiMQCTANu6sZPaaUj5et5O5G8rYV10LQN+sJMb2Smdc73TOyO9ESnyMz5VKU6lrSESOq67esXzrXuasL2PO+p3ML9xFdU09UQaDczoyrldnxvVOZ0SPNDrE6vpCW6MgEJGTdqC2joWb9jBn/U4+XreTJUV7qat3REcZg3NSOT2vE6f37MSo3E6kJuiMobVTEIhIk1UcqGX+xl3MK9zFvI27WFq0h5o6hxn0y0rm9J6dGB0JhyzNoNrqKAhEpNlV19SxeMse5m3cxfzCXSzYtJuqg3UA5HZOYHReJ0bmpjG8R0f6ZCYT0ohnX+muIRFpdvExIc7M78yZ+Z0BqK2rZ8W2fcyPnDG8+2kJLy4oAiAxNsTQ7h0Z0SMcDMO6d9TEea2IzghExBPOOTaVVbFoy24Wbd7Dws27WbW9nLr68HdOXucEhkeCYXj3NPp3TSYmFOVz1e2XzghEpMWZGXnpieSlJ3L58BwA9h+sY9nWvSzcvJtFm3fz0bqdvLJoKwDxMVEM7JbK4OzwY0hOKvkZSepSagE6IxAR3zjn2LpnP4s272HR5j0s27qH5Vv3sb8mfK0hITbEwG4pDM7uyOCc8HN+eqJmWD0FOiMQkVbJzMhJSyAnLYFLhnYDwuMZNpRWsLRoL8u2hh/PzdtE9cf1ACTFRTOgWwpDslMZmJ3CgK6p5GckqlupCRQEItKqhKKMPlnJ9MlK5sqR4S6l2rp61pdWsrRoz+Fw+PPcTRyoDYdDbCiKPllJDOiawmmRx4CuKRrfcILUNSQibVJNXT0bSitZtX0fq7bvY2XkeWfFwcP7ZHfsEAmF5PBztxS6pyUEsmtJXUMi0u7EhKLo1yWZfl2SuWx49uHtJeXVrNy2j1Xbyw8HxLufFhO5WYnE2BC9s5Lpm5lE36xk+mQl0a9LMl1S4jnWkrntmYJARNqVzOR4MvvFM6Ff5uFt1TV1rCkuZ+W2fXy6o5w1xeW8t7qUv0XGOQAkx0XTJ+tQOCTTLyuZvllJZCTHtfuAUBCISLsXHxNiSE5HhuR0/LftuyoPsqa4nLXF5awprmBNcTkzV+zg+flbDu+T2iGGvllJ4esWmUnkZyTRKyORbqkd2k0Xk4JARAKrU2Lsv42OhvAtrTsrDkbCoZw1JRWsLS7nH0u3s3d/zeH94mOiyOucSK/MJHqlJ5KfkUR+Rvg5Ka5tfbW2rWpFRDxmZmQkx5GRHMfY3umHtx8KiA2lFawvrWRDaQUbdlayfOte/rls++FrEABZKXHkpyfRKzOR/PRwQPTKSCK7Y+s8i1AQiIicgIYBcUaDMwgIT9m9uayK9YdDopL1pRW8tnjb4cV+IHyba/dOHcjrnEhu50RyOyeQ2zmBvM6JZKd18G0shIJARKSJ4qJDh8c+NOSco6zyIOtLwmcPhWWVbNpZRWFZJXPWlx0eQQ3h8RM5aR3o0SkhEhSfPXfvlEB8jHeLASkIREQ8YmakJ8WRnvT5swjnHKUVB9hUVkXhzko2lVWxaVcVm8oqeXXxVsobnEmYQdeUeG4Y15Nbzs5v9joVBCIiPjCz8K2uyfGMzuv0b+8559hTVXM4GAp3hp8zU7yZultBICLSypgZaYmxpCXGMqx7x+P/QhNpliYRkYBTEIiIBJyCQEQk4BQEIiIBpyAQEQk4BYGISMApCEREAk5BICIScG1uqUozKwU2neKvpwM7m7GctiKI7Vabg0FtPnG5zrmMo73R5oKgKcysoLE1O9uzILZbbQ4Gtbl5qGtIRCTgFAQiIgEXtCCY7ncBPgliu9XmYFCbm0GgrhGIiMjnBe2MQEREjqAgEBEJuMAEgZlNMrPVZrbOzO73ux6vmFmhmS0zs8VmVhDZ1snMZpnZ2shzmt91NoWZ/cHMSsxseYNtjbbRzL4XOe6rzWyiP1U3TSNt/pGZbY0c68VmdmGD99pDm7ub2XtmtsrMVpjZ3ZHt7fZYH6PN3h5r51y7fwAhYD2QD8QCS4ABftflUVsLgfQjtv0vcH/k5/uB//G7zia28WxgBLD8eG0EBkSOdxzQM/L3IOR3G5qpzT8CvnOUfdtLm7sCIyI/JwNrIm1rt8f6GG329FgH5YzgdGCdc26Dc+4g8Dxwqc81taRLgT9Ffv4TcJmPtTSZc242sOuIzY218VLgeefcAefcRmAd4b8PbUojbW5Me2nzdufcwsjP5cAqIJt2fKyP0ebGNEubgxIE2cCWBq+LOPZ/3LbMAW+Z2QIzmxrZluWc2w7hv2hApm/VeaexNrb3Y3+nmS2NdB0d6iJpd202szxgOPAJATnWR7QZPDzWQQkCO8q29nrf7Djn3AjgAuAOMzvb74J81p6P/eNAL2AYsB14KLK9XbXZzJKAl4B7nHP7jrXrUba1yXYfpc2eHuugBEER0L3B6xxgm0+1eMo5ty3yXAK8Qvg0sdjMugJEnkv8q9AzjbWx3R5751yxc67OOVcPPMFnXQLtps1mFsOIm8kAAANCSURBVEP4C/FZ59zLkc3t+lgfrc1eH+ugBMF8oI+Z9TSzWOBq4DWfa2p2ZpZoZsmHfga+BCwn3NbrI7tdD8zwp0JPNdbG14CrzSzOzHoCfYB5PtTX7A59GUZcTvhYQztps5kZ8BSwyjn3cIO32u2xbqzNnh9rv6+St+DV+AsJX4FfDzzgdz0etTGf8B0ES4AVh9oJdAbeAdZGnjv5XWsT2/kXwqfHNYT/RXTTsdoIPBA57quBC/yuvxnb/GdgGbA08oXQtZ21eTzhbo6lwOLI48L2fKyP0WZPj7WmmBARCbigdA2JiEgjFAQiIgGnIBARCTgFgYhIwCkIREQCTkEggWNmFZHnPDOb0syf/f0jXs9pzs8X8YKCQIIsDzipIDCz0HF2+bcgcM6NPcmaRFqcgkCC7BfAWZH53b9lZiEze9DM5kcm9/oGgJlNiMwR/xzhQT2Y2auRif1WHJrcz8x+AXSIfN6zkW2Hzj4s8tnLLbxexFUNPvt9M3vRzD41s2cjo0sxs1+Y2cpILb9s8f86EhjRfhcg4qP7Cc/xfjFA5At9r3NutJnFAR+b2VuRfU8HBrnwVL8ANzrndplZB2C+mb3knLvfzO50zg07yp91BeEJw4YC6ZHfmR15bzgwkPAcMR8D48xsJeGpBPo755yZdWz21otE6IxA5DNfAq4zs8WEp/7tTHjuFoB5DUIA4C4zWwLMJTzpVx+ObTzwFxeeOKwY+AAY3eCzi1x4QrHFhLus9gHVwJNmdgVQ1eTWiTRCQSDyGQO+6ZwbFnn0dM4dOiOoPLyT2QTgPGCMc24osAiIP4HPbsyBBj/XAdHOuVrCZyEvEV545c2TaonISVAQSJCVE14O8JCZwG2RaYAxs76RWVyPlArsds5VmVl/4MwG79Uc+v0jzAauilyHyCC89GSjs0RG5qNPdc69AdxDuFtJxBO6RiBBthSojXTxPA08SrhbZmHkgm0pR1/W803gVjNbSnjGx7kN3psOLDWzhc65rzXY/gowhvDMsA64zzm3IxIkR5MMzDCzeMJnE986tSaKHJ9mHxURCTh1DYmIBJyCQEQk4BQEIiIBpyAQEQk4BYGISMApCEREAk5BICIScP8H9uZBYLGfL2MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Loading Data\n",
    "\n",
    "\n",
    "x_train = digits_final.loc[:,digits_final.columns != \"target\"]\n",
    "y_train = digits_final.loc[:,digits_final.columns == \"target\"]\n",
    "\n",
    "x_test = digits_test_final.loc[:,digits_test_final.columns != \"target\"]\n",
    "y_test = digits_test_final.loc[:,digits_test_final.columns == \"target\"]\n",
    "\n",
    "\n",
    "#standardize\n",
    "# X[:,0] = X[:,0] / np.amax(X[:,0])\n",
    "# X[:,1] = X[:,1] / np.amax(X[:,1])\n",
    "\n",
    "\n",
    "lr = SoftmaxRegression(learning_rate=0.00001, max_iters=250, minibatches=1)\n",
    "gd = GradientDescent()\n",
    "lr.fit(x_train, y_train.values.ravel(), x_test, y_test.values.ravel(), gd)\n",
    "\n",
    "# #X_plt = X[:, [0,1]]\n",
    "\n",
    "# #plot_decision_regions(X_plt, y, clf=lr)\n",
    "# #plt.title('Softmax Regression - Gradient Descent')\n",
    "# #plt.show()\n",
    "\n",
    "plt.plot(range(len(lr.cost_)), lr.cost_)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1419,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1419-4f0a6917930f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdigits_target_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "digits_target_test.values[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 2 Class Labels: [2 8 2 6 6 7 1 9 8 5 2 8 6 6 6 6 1 0 5 8 8 7 8 4 7 5 4 9 2 9 4 7 6 8 9 4 3\n",
      " 1 0 1 8 6 7 7 9 0 7 6 2 1 9 6 7 9 0 0 9 1 6 3 0 2 3 4 1 9 7 6 9 1 8 3 5 1\n",
      " 2 1 2 2 9 7 2 3 6 0 5 3 7 5 1 2 9 9 3 1 7 7 4 8 5 8 5 5 2 5 9 0 7 1 4 7 3\n",
      " 4 8 9 7 7 8 0 1 9 2 5 8 4 1 7 0 6 1 5 9 9 9 5 9 9 5 7 5 6 2 8 6 9 6 1 5 1\n",
      " 5 9 9 1 5 3 6 1 8 9 7 7 6 7 6 5 6 0 8 8 9 3 6 1 0 7 1 6 3 8 6 7 4 9 6 3 0\n",
      " 3 3 3 0 7 7 5 7 8 0 7 8 9 6 4 5 0 1 4 6 4 3 3 0 9 5 9 3 9 4 7 1 6 8 9 2 4\n",
      " 9 3 7 6 2 3 3 1 6 9 3 6 3 3 2 0 7 6 1 1 3 7 2 7 1 5 5 7 5 2 2 7 2 7 5 5 7\n",
      " 0 9 1 6 5 9 7 4 3 8 0 3 6 4 6 3 2 6 8 8 8 4 6 7 5 2 4 5 3 2 4 6 9 4 5 4 3\n",
      " 4 6 2 9 0 6 7 2 0 9 6 0 4 2 0 7 5 8 5 7 8 2 8 4 3 7 2 6 5 9 5 1 0 8 2 5 9\n",
      " 5 6 8 2 7 2 1 5 1 6 4 5 0 9 4 1 1 7 0 8 9 0 5 4 8 8 8]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = lr.predict(digits_test.values)\n",
    "print('Last 2 Class Labels: %s' % y_pred[0:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1420,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.15534086 1.         ... 1.         0.         1.        ]\n",
      " [0.33333333 0.16950716 1.         ... 1.         0.         1.        ]\n",
      " [0.83333333 0.40208424 1.         ... 0.         0.         1.        ]\n",
      " ...\n",
      " [0.125      0.20798958 0.25       ... 3.         0.         1.        ]\n",
      " [0.25       0.10464611 0.5        ... 3.         0.         1.        ]\n",
      " [0.5        0.53500868 0.25       ... 3.         1.         1.        ]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit() missing 2 required positional arguments: 'y_val' and 'gd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1420-94ef20f86fc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSoftmaxRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlr2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() missing 2 required positional arguments: 'y_val' and 'gd'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading Data\n",
    "\n",
    "x_train = credit_final.loc[:,credit_final.columns != \"class\"]\n",
    "y_train = credit_final.loc[:,credit_final.columns == \"class\"]\n",
    "\n",
    "print (x_train.to_numpy())\n",
    "\n",
    "# df = pd.DataFrame(x_train)\n",
    "# df = df.transform(lambda x: x if (np.amax(x) == 0) else (x / np.amax(x)))\n",
    "\n",
    "lr2 = SoftmaxRegression(learning_rate=0.00001, max_iters=250, minibatches=1)\n",
    "gd = GradientDescent()\n",
    "lr2.fit(x_train.to_numpy(), y_train.values.ravel(), gd)\n",
    "\n",
    "print (x_train)\n",
    "#X_plt = X[:, [0,1]]\n",
    "\n",
    "#plot_decision_regions(X_plt, y, clf=lr)\n",
    "#plt.title('Softmax Regression - Gradient Descent')\n",
    "#plt.show()\n",
    "\n",
    "plt.plot(range(len(lr2.cost_)), lr2.cost_)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "834     bad\n",
       "832     bad\n",
       "435     bad\n",
       "5      good\n",
       "769    good\n",
       "679    good\n",
       "722     bad\n",
       "215    good\n",
       "653     bad\n",
       "150    good\n",
       "Name: class, dtype: category\n",
       "Categories (2, object): [good, bad]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_target_test[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 2 Class Labels: [0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lr2.predict(one_hot_cols_test.values)\n",
    "print('Last 2 Class Labels: %s' % y_pred[-100:])\n",
    "len(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1426,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_Validation:\n",
    "    def cross_validate(self, data, k, model, gradient_obj, test_cols): \n",
    "        \n",
    "        train_col = None\n",
    "        \n",
    "        for c in test_cols:\n",
    "            if c in data:\n",
    "                train_col = c\n",
    "                break\n",
    "        \n",
    "        shuffled_data = data.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "            \n",
    "        folds = np.array_split(shuffled_data, k)\n",
    "        \n",
    "        accuracy_sum = 0\n",
    "        \n",
    "        for i in range(k):\n",
    "            folds_to_train = folds.copy()\n",
    "            fold_to_test = folds_to_train[i]\n",
    "            del folds_to_train[i]\n",
    "            folds_to_train = pd.concat(folds_to_train, sort=False)\n",
    "            \n",
    "            x_train = folds_to_train.loc[:,folds_to_train.columns != train_col]\n",
    "            y_train = folds_to_train.loc[:,folds_to_train.columns == train_col]\n",
    "            \n",
    "            x_test = fold_to_test.loc[:,fold_to_test.columns != train_col]\n",
    "            y_test = fold_to_test.loc[:,fold_to_test.columns == train_col]\n",
    "            \n",
    "            model.fit(x_train, y_train.values.ravel(), x_test, y_test.values.ravel(), gradient_obj)\n",
    "            predictions = model.predict(x_test.to_numpy())\n",
    "            accuracy_sum += accuracy_score(predictions, y_test.values.ravel())\n",
    "            \n",
    "        return accuracy_sum / k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "# set up grid testing\n",
    "\n",
    "\n",
    "# param_grid = [\n",
    "#   {'learning_rate': np.arange(0.00001, 0.0001, 0.00005), 'max_iters': [250], 'random_seed':[0],\n",
    "#   'alphaa': np.arange(0.001, 0.011, 0.01), 'beta1': np.arange(0.9, 0.99, 0.09), 'max_iterations': [1e4], 'max_no_change': np.arange(10,20,10), \n",
    "#   'adaptive': [False], 'beta2': np.arange(0.99, 0.999, 0.009), 'epsilon': np.arange(1e-9, 1e-8, 9e-9), 'minibatch_size': np.arange(1,10, 9),\n",
    "#   'regularize': [0,1,2], 'lambdaa': np.arange(0.01, 0.1, 0.09)}\n",
    "# ]\n",
    "\n",
    "\n",
    "#merge train and train targets, send to cross validation for each grid\n",
    "# print (digits_final)\n",
    "# print (credit_final)\n",
    "\n",
    "class Tester:\n",
    "    def __init__(self, learning_rate=0.01, max_iters=50, alphaa=0.001, beta1=0.9, max_iterations=1e4, max_no_change=20,\n",
    "                 adaptive=False, beta2=0.999, epsilon=1e-8, minibatch_size=0, regularize=0, lambdaa=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.alphaa = alphaa\n",
    "        self.beta1 = beta1\n",
    "        self.max_iterations = max_iterations\n",
    "        self.max_no_change = max_no_change\n",
    "        self.adaptive = adaptive\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.regularize = regularize\n",
    "        self.lambdaa = lambdaa\n",
    "        self.minibatch_size = minibatch_size    \n",
    "    \n",
    "#     def get_params(self, deep=True):\n",
    "#         # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
    "#         params = {'learning_rate': self.learning_rate, 'max_iters': self.max_iters, 'random_seed':self.random_seed,\n",
    "#                   'alphaa': self.alphaa, 'beta1': self.beta1, 'max_iterations': self.max_iterations, 'max_no_change': self.max_no_change, \n",
    "#                   'adaptive': self.adaptive, 'beta2':self.beta2, 'epsilon': self.epsilon, 'minibatch_size': self.minibatch_size,\n",
    "#                   'regularize': self.regularize, 'lambdaa': self.lambdaa}\n",
    "#         return params\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "    def fit(self,X):\n",
    "        self.lr = SoftmaxRegression(learning_rate=self.learning_rate, max_iters=self.max_iters)\n",
    "        self.gd = GradientDescent(alphaa=self.alphaa, max_iterations=self.max_iterations, max_no_change=self.max_no_change,\n",
    "                                  adaptive=self.adaptive, beta2=self.beta2, epsilon=self.epsilon, regularize=self.regularize,\n",
    "                                  lambdaa=self.lambdaa, minibatch_size=self.minibatch_size)\n",
    "        self.accuracy = cross.cross_validate(X, 5, self.lr, self.gd, ['target', 'class'])\n",
    "        print (self.accuracy)\n",
    "        return self.accuracy\n",
    "        \n",
    "    \n",
    "# def test_scorer(estimator, X):\n",
    "#     print (estimator.accuracy)\n",
    "#     return estimator.accuracy\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "max_iters = [250]\n",
    "alphas = [0.01, 0.1]\n",
    "beta1s = [0.99, 0.9]\n",
    "max_iterations = [1e4]\n",
    "max_no_changes = [20]\n",
    "adaptives = [False]\n",
    "beta2s = [0.99]\n",
    "epsilons = [1e-8]\n",
    "minibatch_sizes = minibatch_sizes = [1,10]\n",
    "regularizes = [0]\n",
    "lambdas = [0.1]\n",
    "\n",
    "combos = np.array(np.meshgrid(learning_rates, max_iters, alphas, beta1s, max_iterations, max_no_changes, adaptives, beta2s,\n",
    "               epsilons, minibatch_sizes, regularizes, lambdas)).T.reshape(-1, 12)\n",
    "\n",
    "print (len(combos))\n",
    "cross = Cross_Validation()\n",
    "\n",
    "class GridSearcher:    \n",
    "    def grid_search(self, data, combinations):\n",
    "        t = None\n",
    "        max_accuracy = 0\n",
    "        min_time = 0\n",
    "\n",
    "        self.accuracies = []\n",
    "        self.train_times = []\n",
    "\n",
    "        for row in combos:\n",
    "            if t is None:\n",
    "                t = Tester(learning_rate=row[0], max_iters=int(row[1]), alphaa=row[2], beta1=row[3], max_iterations=int(row[4]),\n",
    "                          max_no_change=int(row[5]), adaptive=row[6], beta2=row[7], epsilon=row[8], minibatch_size=int(row[9]), regularize=int(row[10]),\n",
    "                          lambdaa=row[11])\n",
    "            else:\n",
    "                t.set_params(learning_rate=row[0], max_iters=int(row[1]), alphaa=row[2], beta1=row[3], max_iterations=int(row[4]),\n",
    "                          max_no_change=int(row[5]), adaptive=row[6], beta2=row[7], epsilon=row[8], minibatch_size=int(row[9]), regularize=int(row[10]),\n",
    "                          lambdaa=row[11])\n",
    "            start_time = time.time()\n",
    "            accuracy = t.fit(data)\n",
    "            run_time = time.time() - start_time\n",
    "            \n",
    "            self.accuracies.append(accuracy)\n",
    "            self.train_times.append(run_time)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def tester_scorer_credit(estimator, X):\n",
    "#     error = cross.cross_validate(X, 5, estimator.lr, estimator.gd, 'class')\n",
    "#     return error\n",
    "\n",
    "# cv = [(slice(None), slice(None))] # dont use grid search cross validation, want to use our own\n",
    "# gs = GridSearchCV(estimator=Tester(), param_grid=param_grid, \n",
    "#                   scoring=test_scorer, cv=cv, n_jobs=-1)\n",
    "\n",
    "# gs.fit(digits_final)\n",
    "#get best hyper parameters, then run test data using them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = GridSearcher()\n",
    "g2.grid_search(credit_final, combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.90e-01 1.00e-09 1.00e+00 0.00e+00 1.00e-02]\n",
      " [6.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.90e-01 1.00e-09 1.00e+00 0.00e+00 1.00e-02]\n",
      " [1.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.99e-01 1.00e-09 1.00e+00 0.00e+00 1.00e-02]\n",
      " [6.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.99e-01 1.00e-09 1.00e+00 0.00e+00 1.00e-02]\n",
      " [1.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.90e-01 1.00e-09 1.00e+00 1.00e+00 1.00e-02]\n",
      " [6.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.90e-01 1.00e-09 1.00e+00 1.00e+00 1.00e-02]\n",
      " [1.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.99e-01 1.00e-09 1.00e+00 1.00e+00 1.00e-02]\n",
      " [6.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.99e-01 1.00e-09 1.00e+00 1.00e+00 1.00e-02]\n",
      " [1.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.90e-01 1.00e-09 1.00e+00 2.00e+00 1.00e-02]\n",
      " [6.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.90e-01 1.00e-09 1.00e+00 2.00e+00 1.00e-02]\n",
      " [1.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.99e-01 1.00e-09 1.00e+00 2.00e+00 1.00e-02]\n",
      " [6.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.99e-01 1.00e-09 1.00e+00 2.00e+00 1.00e-02]\n",
      " [1.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.90e-01 1.00e-09 1.00e+00 0.00e+00 1.00e-01]\n",
      " [6.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.90e-01 1.00e-09 1.00e+00 0.00e+00 1.00e-01]\n",
      " [1.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.99e-01 1.00e-09 1.00e+00 0.00e+00 1.00e-01]\n",
      " [6.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.99e-01 1.00e-09 1.00e+00 0.00e+00 1.00e-01]\n",
      " [1.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.90e-01 1.00e-09 1.00e+00 1.00e+00 1.00e-01]\n",
      " [6.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.90e-01 1.00e-09 1.00e+00 1.00e+00 1.00e-01]\n",
      " [1.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.99e-01 1.00e-09 1.00e+00 1.00e+00 1.00e-01]\n",
      " [6.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.99e-01 1.00e-09 1.00e+00 1.00e+00 1.00e-01]\n",
      " [1.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.90e-01 1.00e-09 1.00e+00 2.00e+00 1.00e-01]\n",
      " [6.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.90e-01 1.00e-09 1.00e+00 2.00e+00 1.00e-01]\n",
      " [1.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.99e-01 1.00e-09 1.00e+00 2.00e+00 1.00e-01]\n",
      " [6.00e-05 2.50e+02 0.00e+00 1.00e-03 9.00e-01 1.00e+04 1.00e+01 0.00e+00\n",
      "  9.99e-01 1.00e-09 1.00e+00 2.00e+00 1.00e-01]]\n"
     ]
    }
   ],
   "source": [
    "g = GridSearcher()\n",
    "g.grid_search(digits_final, combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
